{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72b14a6",
   "metadata": {},
   "source": [
    "# Postoperative AKI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e27167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import scipy.stats as stats\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "shap.initjs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d538318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"final_data.xlsx\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026ef62",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cb10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hight and weight should be float or int instead of object\n",
    "df['Weight'] = pd.to_numeric(df['Weight'], errors='coerce')\n",
    "df['Height'] = pd.to_numeric(df['Height'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe96f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe().T\n",
    "\n",
    "# we can see many outliers in height and weight, we will deal with them by visualizing the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the postive samples\n",
    "df['aki'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b55ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['aki'] == 1]['Surgery type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "       # delete time related columns\n",
    "       'Surgery begin time', 'Surgery end time', 'Test time',\n",
    "       # delete description columns\n",
    "       'Surgery name']\n",
    "\n",
    "df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294468ff",
   "metadata": {},
   "source": [
    "## Delete Samples with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values for negative samples\n",
    "\n",
    "def missing_rows(df):\n",
    "    '''\n",
    "    calculate the missing values of each row\n",
    "    input:\n",
    "        df: the dataframe\n",
    "    output:\n",
    "        missing_percent: the missing percentage of each row\n",
    "        missing_count: the number of missing values of each row\n",
    "    '''\n",
    "    missing_row = df.isna()\n",
    "    missing_row = missing_row[missing_row.any(axis=1)]\n",
    "    missing_percent = missing_row.mean(axis=1)\n",
    "    missing_count = missing_percent.value_counts()\n",
    "    return missing_percent, missing_count\n",
    "\n",
    "missing_percent0, missing_count0 = missing_rows(df[df['aki'] == 0])\n",
    "print('Missing percentage of negative samples:')\n",
    "print(missing_count0)\n",
    "print('total number of negative samples with missing values:', sum(missing_count0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the negative rows with missing values\n",
    "df = df.drop(missing_percent0.index, axis=0)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values for positive samples\n",
    "\n",
    "missing_percent1, missing_count1 = missing_rows(df[df['aki'] == 1])\n",
    "print('Missing percentage of positive samples:')\n",
    "print(missing_count1)\n",
    "print('total number of positive samples with missing values:', sum(missing_count1))\n",
    "\n",
    "# drop the rows with missing percentage > 0.2\n",
    "to_drop = missing_percent1[missing_percent1 > 0.2].index\n",
    "print(f'drop the rows with missing percentage > 0.2: {len(to_drop)} rows')\n",
    "df.drop(to_drop, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def99316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_columns(df):\n",
    "    \"\"\"\n",
    "    Return the missing values of each column\n",
    "    input:\n",
    "        df: the dataframe\n",
    "    output:\n",
    "        missing_values: the missing count and percentage of each column\n",
    "    \"\"\"\n",
    "    missing_number = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_percent = (df.isnull().sum()/df[df['aki'] == 1].shape[0]).sort_values(ascending=False)\n",
    "    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n",
    "    missing_values = missing_values[missing_values['Missing_Number'] > 0]\n",
    "    if missing_values.shape[0] == 0:\n",
    "        print('No missing values')\n",
    "    else:\n",
    "        return missing_values\n",
    "    \n",
    "missing_col = missing_columns(df)\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80323076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column with missing percent > 0.2\n",
    "dropped_col = missing_col[missing_col['Missing_Percent'] > 0.2].index\n",
    "print(dropped_col)\n",
    "df.drop(dropped_col, axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60652b",
   "metadata": {},
   "source": [
    "## Delete Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac79495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes('object').columns #discrete\n",
    "df.select_dtypes('int64').columns #continuous\n",
    "df.select_dtypes('float64').columns #continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes('float64').columns\n",
    "\n",
    "# Define the number of rows and columns for the subplots\n",
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each numeric column in a subplot\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.histplot(df[col], bins=30, kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13c6955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each numerical column, find the highest and lowest 10 values and the corresponding 'aki'\n",
    "\n",
    "outliers = pd.DataFrame()\n",
    "for col in numeric_cols:\n",
    "    highest_10 = df[[col, 'aki']].nlargest(10, col)\n",
    "    lowest_10 = df[[col, 'aki']].nsmallest(10, col)\n",
    "    \n",
    "    highest_10['feature'] = col\n",
    "    highest_10['type'] = 'highest'\n",
    "    highest_10 = highest_10.rename(columns={col: 'value'})\n",
    "    \n",
    "    lowest_10['feature'] = col\n",
    "    lowest_10['type'] = 'lowest'\n",
    "    lowest_10 = lowest_10.rename(columns={col: 'value'})\n",
    "    \n",
    "    outliers = pd.concat([outliers, highest_10, lowest_10])\n",
    "\n",
    "outliers = outliers[['feature', 'type', 'value', 'aki']]\n",
    "# outliers.to_excel('outliers.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with outliers\n",
    "\n",
    "# drop height higher than 200 or lower than 100 and print number of dropped rows\n",
    "to_drop_height = df[(df['Height'] > 200) | (df['Height'] < 100)]\n",
    "print(f'Number of dropped rows due to height outliers: {to_drop_height.shape[0]}')\n",
    "df.drop(to_drop_height.index, inplace=True)\n",
    "\n",
    "# drop the rows with weight less than 40\n",
    "to_drop_weight = df[df['Weight'] < 40]\n",
    "print(f'Number of dropped rows due to weight outliers: {to_drop_weight.shape[0]}')\n",
    "df.drop(to_drop_weight.index, inplace=True)\n",
    "\n",
    "# delete the rows whose surgery time is less than 10 minues\n",
    "to_drop_time = df[df['Operation time (mins)'] < 10]\n",
    "print(f'Number of dropped rows due to operation time outliers: {to_drop_time.shape[0]}')\n",
    "df.drop(to_drop_time.index, inplace=True)\n",
    "\n",
    "to_drop_bmi = df[(df['BMI'] < 10) | (df['BMI'] > 50)]\n",
    "print(f'Number of dropped rows due to BMI outliers: {to_drop_bmi.shape[0]}')\n",
    "df.drop(to_drop_bmi.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9ac19",
   "metadata": {},
   "source": [
    "## Fill in NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61cc1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weight = df['Weight'].mean()\n",
    "df['Weight'] = df['Weight'].fillna(average_weight)\n",
    "\n",
    "average_height = df['Height'].mean()\n",
    "df['Height'] = df['Height'].fillna(average_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5758bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bmi(row):\n",
    "    height = row['Height']\n",
    "    weight = row['Weight']\n",
    "    if pd.isna(height) or pd.isna(weight):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return weight / ((height / 100) ** 2)\n",
    "\n",
    "df['BMI'] = df.apply(calculate_bmi, axis=1)  \n",
    "df.drop(['Height', 'Weight'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03faf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use knn to fill in NA value for positive samples\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "numeric_for_impute = ['Age (years)', 'BMI', 'Serum creatinine (μmol/L)', 'Serum albumin (g/L)', 'Serum carbon dioxide (mmol/L)', 'Urea (mmol/L)',\n",
    "       'Serum potassium (mmol/L)', 'Serum sodium (mmol/L)', 'Neutrophils (*10^9/L)', 'Lymphocyte (*10^9/L)',\n",
    "       'Monocyte (*10^9/L)', 'Platelet (*10^9/L)', 'NLR', 'PLR', 'PNI', 'MLR', 'SII', 'UCR']\n",
    "\n",
    "df_b4_impute = df.copy()\n",
    "\n",
    "# Encode the categorical column \"Gender\"\n",
    "encoder = preprocessing.OrdinalEncoder()\n",
    "df1 = df[df['aki'] == 1].copy()\n",
    "df1['Gender_encoded'] = encoder.fit_transform(df1[['Gender']])\n",
    "\n",
    "# Combine the numeric columns with the encoded \"Gender\" column\n",
    "columns_for_imputation = numeric_for_impute + ['Gender_encoded']\n",
    "df1_for_imputation = df1[columns_for_imputation]\n",
    "\n",
    "# Apply KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df1_imputed = pd.DataFrame(imputer.fit_transform(df1_for_imputation), columns=columns_for_imputation)\n",
    "\n",
    "# Round the imputed \"Gender\" column and convert to int\n",
    "df1_imputed['Gender_encoded'] = np.round(df1_imputed['Gender_encoded']).astype(int)\n",
    "\n",
    "# Decode the imputed \"Gender\" column\n",
    "df1['Gender_imputed'] = encoder.inverse_transform(df1_imputed[['Gender_encoded']]).flatten()\n",
    "\n",
    "# Update the original dataframe with the imputed values\n",
    "df1_imputed.index = df1.index\n",
    "df1.update(df1_imputed[numeric_for_impute])\n",
    "df1['Gender'] = df1['Gender_imputed']\n",
    "\n",
    "# Update the original dataframe\n",
    "df.update(df1)\n",
    "\n",
    "# Drop the temporary columns\n",
    "df1.drop(columns=['Gender_encoded', 'Gender_imputed'], inplace=True)\n",
    "\n",
    "# Create a comparison DataFrame to show the differences\n",
    "comparison_df = df_b4_impute.copy()\n",
    "comparison_df['Gender_imputed'] = df['Gender']\n",
    "for col in numeric_for_impute:\n",
    "    comparison_df[col + '_imputed'] = df[col]\n",
    "\n",
    "# Create the desired column order with original columns next to imputed columns\n",
    "ordered_columns = []\n",
    "\n",
    "ordered_columns.append('Gender')\n",
    "ordered_columns.append('Gender_imputed')\n",
    "\n",
    "for col in numeric_for_impute:\n",
    "    ordered_columns.append(col)\n",
    "    ordered_columns.append(col + '_imputed')\n",
    "\n",
    "# Reorder the columns in the comparison DataFrame\n",
    "comparison_df = comparison_df[ordered_columns]\n",
    "\n",
    "comparison_df_highlight = comparison_df[(df_b4_impute != df).any(axis=1)]\n",
    "\n",
    "# comparison_df_highlight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b53b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d10812",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd20b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes('object').columns #discrete\n",
    "df.select_dtypes('int64').columns #continuous\n",
    "df.select_dtypes('float64').columns #continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns with surgery type and operation time\n",
    "category_cols =['Surgery type', 'Gender', 'Hypertension', 'Diabetes mellitus', 'Coronary artery disease', 'COPD', 'Cancer']\n",
    "\n",
    "numeric_cols =['Operation time (mins)', 'Age (years)', 'BMI', 'Serum creatinine (μmol/L)',\n",
    "       'Serum albumin (g/L)', 'Serum carbon dioxide (mmol/L)', 'Urea (mmol/L)',\n",
    "       'Serum potassium (mmol/L)', 'Serum sodium (mmol/L)',\n",
    "       'Neutrophils (*10^9/L)', 'Lymphocyte (*10^9/L)', 'Monocyte (*10^9/L)',\n",
    "       'Platelet (*10^9/L)', 'NLR', 'PLR', 'PNI', 'MLR', 'SII',\n",
    "       'UCR']\n",
    "\n",
    "target = 'aki'\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "original_df = df.copy()\n",
    "\n",
    "df = df[category_cols + numeric_cols + [target]]\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eca0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[category_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a02cc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Surgery type'] = df['Surgery type'].map({'TEVAR': 1, 'EVAR': 0, 'OSR': 2})\n",
    "df['Gender'] = df['Gender'].map({'M': 1, 'F': 0})\n",
    "df['Hypertension'] = df['Hypertension'].map({'Y': 1, 'N': 0})\n",
    "df['Diabetes mellitus'] = df['Diabetes mellitus'].map({'Y': 1, 'N': 0})\n",
    "df['Coronary artery disease'] = df['Coronary artery disease'].map({'Y': 1, 'N': 0})\n",
    "df['COPD'] = df['COPD'].map({'Y': 1, 'N': 0})\n",
    "df['Cancer'] = df['Cancer'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# df[category_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd80f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distplots without any transformation\n",
    "# To know the scale of how skewed the data is we can plot a probability distribution plot that exactly describes what transformations are needed to be performed\n",
    "\n",
    "def plot_feature_distribution(df, numeric_cols):\n",
    "    '''\n",
    "    plot the distribution of each numeric column\n",
    "    input:\n",
    "        df: the dataframe\n",
    "        numeric_cols: the numeric columns\n",
    "    output:\n",
    "        print (1) the distribution of each numeric column and the proportion of positive samples in each bin\n",
    "        print (2) the probability plot of each numeric column\n",
    "    '''\n",
    "    # Define the number of rows and columns for the subplots\n",
    "    n_cols = 1\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
    "    bin_count = 15\n",
    "    font_size = 20\n",
    "\n",
    "    # Create a figure and a grid of subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols * 2, figsize=(20, n_rows * 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each numeric column in a subplot\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        # First subplot: Histogram\n",
    "        sns.histplot(df[col], bins=bin_count, kde=True, ax=axes[2 * i])\n",
    "        axes[2 * i].set_title(f'Distribution of {col}', fontsize=font_size)\n",
    "        axes[2 * i].set_xlabel(col, fontsize=font_size)\n",
    "        axes[2 * i].set_ylabel('Frequency', fontsize=font_size)\n",
    "        axes[2 * i].tick_params(labelsize=font_size)\n",
    "        \n",
    "        # Calculate the proportion of positive samples in each bin\n",
    "        bins = np.linspace(df[col].min(), df[col].max(), bin_count+1)\n",
    "        df['bin'] = pd.cut(df[col], bins=bins, include_lowest=True)\n",
    "        bin_mean = df.groupby('bin')['aki'].mean()\n",
    "        bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "        # Overlay the proportion of positive samples\n",
    "        ax2 = axes[2 * i].twinx()\n",
    "        ax2.plot(bin_centers, bin_mean, 'r-', marker='o')\n",
    "        ax2.set_ylabel('Proportion of Positive Samples', color='r', fontsize=font_size)\n",
    "        ax2.tick_params(axis='y', labelcolor='r', labelsize=font_size)\n",
    "\n",
    "        # Extract the color used in the histogram plot\n",
    "        color = axes[2 * i].patches[0].get_facecolor()\n",
    "        \n",
    "        # Second subplot: Probability plot\n",
    "        (osm, osr), (slope, intercept, r) = stats.probplot(df[col], dist=\"norm\")\n",
    "        axes[2 * i + 1].plot(osm, osr, 'o', color=color)  # Use the extracted color\n",
    "        axes[2 * i + 1].plot(osm, slope * osm + intercept, 'b-')\n",
    "        axes[2 * i + 1].set_title(f'Probability Plot of {col}', fontsize=font_size)\n",
    "        axes[2 * i + 1].tick_params(labelsize=font_size)\n",
    "\n",
    "\n",
    "\n",
    "    # Remove any empty subplots\n",
    "    for j in range(2 * i + 2, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    df.drop(columns=['bin'], inplace=True)\n",
    "\n",
    "# Example usage\n",
    "plot_feature_distribution(df, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ed54af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use log1p transformation for some of the skewed features\n",
    "\n",
    "skewed = ['Operation time (mins)', 'Serum creatinine (μmol/L)', 'Urea (mmol/L)', 'Neutrophils (*10^9/L)', 'Lymphocyte (*10^9/L)', 'Monocyte (*10^9/L)', 'Platelet (*10^9/L)', 'NLR', 'PLR', 'MLR', 'SII']\n",
    "\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log1p', preprocessing.FunctionTransformer(np.log1p), skewed)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply the transformer\n",
    "transformed_data = transformer.fit_transform(df)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=skewed + [col for col in df.columns if col not in skewed])\n",
    "\n",
    "# Ensure the column order matches the original DataFrame\n",
    "df = transformed_df[df.columns]\n",
    "\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(df, numeric_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62291d2",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc208e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6131422",
   "metadata": {},
   "outputs": [],
   "source": [
    "intraoperative_features = ['Operation time (mins)', 'Surgery type']\n",
    "intraoperative = df[intraoperative_features].copy()\n",
    "X = df.drop(columns=[target]+intraoperative_features).copy()\n",
    "y = df[target].copy()\n",
    "new_X = pd.concat([X, intraoperative], axis=1)\n",
    "numeric_noTime = [col for col in numeric_cols if col != 'Operation time (mins)']\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.value_counts())\n",
    "# X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f971c96",
   "metadata": {},
   "source": [
    "## Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77ed2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train test split\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "def test_splitting(original_df, train_index, test_index):\n",
    "\n",
    "    '''\n",
    "    use mannwhitneyu test for continuous features and chi square test for categorical features to test whether the distribution of the training set and testing set are different\n",
    "    if p value < 0.05, then the distribution of the training set and testing set are significantly different\n",
    "    assume the data is not normally distributed\n",
    "    input:\n",
    "        original_df: the original dataframe\n",
    "        train_index: the index of the training set\n",
    "        test_index: the index of the testing set\n",
    "    output:\n",
    "        True/False, spreadsheet_df\n",
    "    '''\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    train_set = original_df.iloc[train_index, :]\n",
    "    test_set = original_df.iloc[test_index, :]\n",
    "\n",
    "    spreadsheet = []\n",
    "\n",
    "    # Patient population row\n",
    "    spreadsheet.append({\n",
    "        'Feature': 'Patient Population',\n",
    "        'All': len(original_df),\n",
    "        'Training Set': len(train_set),\n",
    "        'Testing Set': len(test_set),\n",
    "        'P-value': ''\n",
    "    })\n",
    "\n",
    "    # Process categorical columns\n",
    "    for col in category_cols:\n",
    "        if col != 'Surgery type':\n",
    "            counts_all = original_df[col].value_counts()\n",
    "            counts_train = train_set[col].value_counts()\n",
    "            counts_test = test_set[col].value_counts()\n",
    "            \n",
    "            if col == 'Gender':\n",
    "                cat = 'M'\n",
    "            else:\n",
    "                cat = 'Y'\n",
    "            count_all = counts_all.get(cat, 0)\n",
    "            count_train = counts_train.get(cat, 0)\n",
    "            count_test = counts_test.get(cat, 0)\n",
    "            if count_all == 0:\n",
    "                p_value = 1\n",
    "            else:\n",
    "                p_value = chi2_contingency(\n",
    "                    [[count_train, len(train_set) - count_train],\n",
    "                        [count_test, len(test_set) - count_test]]\n",
    "                )[1]\n",
    "            spreadsheet.append({\n",
    "                'Feature': f\"{col} ({cat})\",\n",
    "                'All': f\"{count_all} ({count_all/len(df)*100:.2f})\",\n",
    "                'Training Set': f\"{count_train} ({count_train/len(train_set)*100:.2f})\",\n",
    "                'Testing Set': f\"{count_test} ({count_test/len(test_set)*100:.2f})\",\n",
    "                'P-value': f\"{p_value:.4f}\"\n",
    "            })\n",
    "        \n",
    "        # surgery type have 3 categories, we need to deal with it seperately\n",
    "        else:\n",
    "            counts_all = original_df[col].value_counts()\n",
    "            counts_train = train_set[col].value_counts()\n",
    "            counts_test = test_set[col].value_counts()\n",
    "            \n",
    "            for cat in counts_all.index:\n",
    "                count_all = counts_all[cat]\n",
    "                count_train = counts_train.get(cat, 0)\n",
    "                count_test = counts_test.get(cat, 0)\n",
    "                p_value = chi2_contingency(\n",
    "                    [[count_train, len(train_set) - count_train],\n",
    "                    [count_test, len(test_set) - count_test]]\n",
    "                )[1]\n",
    "                spreadsheet.append({\n",
    "                    'Feature': f\"{col} ({cat})\",\n",
    "                    'All': f\"{count_all} ({count_all/len(df)*100:.2f})\",\n",
    "                    'Training Set': f\"{count_train} ({count_train/len(train_set)*100:.2f})\",\n",
    "                    'Testing Set': f\"{count_test} ({count_test/len(test_set)*100:.2f})\",\n",
    "                    'P-value': f\"{p_value:.4f}\"\n",
    "                })\n",
    "\n",
    "    # Process numerical columns\n",
    "    for col in numeric_cols:\n",
    "        all_med = original_df[col].median()\n",
    "        all_min = original_df[col].min()\n",
    "        all_max = original_df[col].max()\n",
    "        train_med = train_set[col].median()\n",
    "        train_min = train_set[col].min()\n",
    "        train_max = train_set[col].max()\n",
    "        test_med = test_set[col].median()\n",
    "        test_min = test_set[col].min()\n",
    "        test_max = test_set[col].max()\n",
    "        \n",
    "        p_value = mannwhitneyu(train_set[col], test_set[col])[1]\n",
    "        \n",
    "        spreadsheet.append({\n",
    "            'Feature': col,\n",
    "            'All': f\"{all_med:.2f} ({all_min:.2f}-{all_max:.2f})\",\n",
    "            'Training Set': f\"{train_med:.2f} ({train_min:.2f}-{train_max:.2f})\",\n",
    "            'Testing Set': f\"{test_med:.2f} ({test_min:.2f}-{test_max:.2f})\",\n",
    "            'P-value': f\"{p_value:.4f}\"\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame for display or export\n",
    "    spreadsheet_df = pd.DataFrame(spreadsheet)\n",
    "\n",
    "    # check whether there's any p value less than 0.05 and ignore '' string\n",
    "    significant_df = spreadsheet_df[spreadsheet_df['P-value'].apply(lambda x: x != '' and float(x) < 0.05)]\n",
    "    significant = not significant_df.empty\n",
    "    \n",
    "    return significant, spreadsheet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50a87f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_sample(y_test, y_prob):\n",
    "    '''\n",
    "    calculate the metrics for the test set\n",
    "    input:\n",
    "        y_test: the true labels\n",
    "        y_prob: the predicted probabilities\n",
    "    output:\n",
    "        auc, recall, precision, accuracy, optimal_threshold, y_pred\n",
    "    '''\n",
    "\n",
    "    # Determine the optimal threshold using Youden's J Statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred = (y_prob > optimal_threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return auc, recall, precision, accuracy, optimal_threshold, y_pred\n",
    "\n",
    "def save_ci(boostrap, alpha):\n",
    "    '''\n",
    "    save the confidence interval in the format of (lower, upper)\n",
    "    input:\n",
    "        boostrap: the bootstrapped metrics\n",
    "        alpha: the significance level\n",
    "    output:\n",
    "        the confidence interval in the format of (lower, upper)\n",
    "    '''\n",
    "    lower, upper = np.percentile(boostrap, [(alpha / 2) * 100, (1 - alpha / 2) * 100])\n",
    "    return str('(' + str(round(lower, 3)) + '-' + str(round(upper, 3)) + ')')\n",
    "\n",
    "def calculate_metrics(y_test, y_prob, ci=False, n_bootstrap=1000, alpha=0.05):\n",
    "\n",
    "    '''\n",
    "    calculate the metrics for the test set\n",
    "    input:\n",
    "        y_test: the true labels\n",
    "        y_prob: the predicted probabilities\n",
    "        ci: whether to calculate the confidence interval, by default is False\n",
    "        n_bootstrap: the number of bootstrap samples, by default is 1000\n",
    "        alpha: the significance level, by default is 0.05\n",
    "    output:\n",
    "        the metrics result as a dictionary, y_pred\n",
    "    '''\n",
    "    metrics_result = {}\n",
    "\n",
    "    # Calculate metrics for the original dataset\n",
    "    auc, recall, precision, accuracy, optimal_threshold, y_pred = calculate_sample(y_test, y_prob)\n",
    "\n",
    "    # Store the original metrics\n",
    "    metrics_result['Cutoff'] = optimal_threshold\n",
    "    metrics_result['AUC'] = auc\n",
    "    metrics_result['Recall'] = recall\n",
    "    metrics_result['Precision'] = precision\n",
    "    metrics_result['Accuracy'] = accuracy\n",
    "\n",
    "    if ci:\n",
    "        # Initialize lists to store bootstrapped metric results\n",
    "        auc_bootstrap = []\n",
    "        recall_bootstrap = []\n",
    "        precision_bootstrap = []\n",
    "        accuracy_bootstrap = []\n",
    "\n",
    "        # Perform bootstrapping\n",
    "        for seed in range(n_bootstrap):\n",
    "            # Generate a bootstrapped sample\n",
    "            np.random.seed(seed)\n",
    "            indices = np.random.choice(len(y_test), size=len(y_test), replace=True)\n",
    "            y_test = y_test.reset_index(drop=True)\n",
    "            y_test_bootstrap = y_test[indices]\n",
    "            y_prob_bootstrap = y_prob[indices]\n",
    "\n",
    "            # Compute metrics for the bootstrapped sample\n",
    "            auc_b, recall_b, precision_b, accuracy_b, _, _ = calculate_sample(y_test_bootstrap, y_prob_bootstrap)\n",
    "            auc_bootstrap.append(auc_b)\n",
    "            recall_bootstrap.append(recall_b)\n",
    "            precision_bootstrap.append(precision_b)\n",
    "            accuracy_bootstrap.append(accuracy_b)\n",
    "\n",
    "        # Calculate confidence intervals and store in the format [lower, upper]\n",
    "        metrics_result['AUC_CI'] = save_ci(auc_bootstrap, alpha)\n",
    "        metrics_result['Recall_CI'] = save_ci(recall_bootstrap, alpha)\n",
    "        metrics_result['Precision_CI'] = save_ci(precision_bootstrap, alpha)\n",
    "        metrics_result['Accuracy_CI'] = save_ci(accuracy_bootstrap, alpha)\n",
    "\n",
    "    return metrics_result, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6134b",
   "metadata": {},
   "source": [
    "## TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cea7294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import random\n",
    "\n",
    "# Define the TensorFlow model\n",
    "def create_model(input_dim, hidden_dim1, hidden_dim2, dropout_rate, l2_reg):\n",
    "    model = Sequential([\n",
    "        Dense(hidden_dim1, input_dim=input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(hidden_dim2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def assess_mlp(X_train, X_test, y_train, y_test, selected_features, ci=False, plot_roc=False, calculate_shap=False, by_category=False):\n",
    "    '''\n",
    "    by default, the function will only return [0] metrics result\n",
    "    ci: whether to calculate the confidence interval\n",
    "    plot_roc: whether to plot the ROC curve\n",
    "    calculate_shap: whether to calculate the SHAP values, if true, will return:\n",
    "        [0] metrics result\n",
    "        [1] shap values\n",
    "        [2] base value\n",
    "        [3] y_pred\n",
    "    by_category: whether to calculate the AUC by each category, if true, will return:\n",
    "        [0] metrics result\n",
    "        [1] auc by each category\n",
    "    if calculate_shap and by_category, will return:\n",
    "        [0] metrics result\n",
    "        [1] shap values\n",
    "        [2] base value\n",
    "        [3] y_pred\n",
    "        [4] auc by each category\n",
    "    '''\n",
    "\n",
    "    seed = 42\n",
    "    # Fixing the random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Parameters\n",
    "    hidden_dim1 = 16\n",
    "    hidden_dim2 = 16\n",
    "    dropout_rate = 0.008\n",
    "    l2_reg = 0.001\n",
    "    batch_size = 48\n",
    "    epochs = 500\n",
    "    learning_rate = 0.001\n",
    "\n",
    "\n",
    "    auc_by_category = {0: [], 1: [], 2: []}\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X_train.loc[:, selected_features], X_test.loc[:, selected_features]\n",
    "\n",
    "    # Create and train the model\n",
    "    model = create_model(input_dim=X_train.shape[1], hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, dropout_rate=dropout_rate, l2_reg=l2_reg)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=BinaryCrossentropy(), metrics=['AUC'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict and calculate AUC\n",
    "    y_prob = model.predict(X_test, verbose=0).ravel()\n",
    "\n",
    "    if by_category:\n",
    "        for category in auc_by_category:\n",
    "            test_mask = X_test['Surgery type'] == category\n",
    "            train_mask = X_train['Surgery type'] == category\n",
    "\n",
    "            if np.any(test_mask):\n",
    "                X_test_filtered = X_test[test_mask]\n",
    "                y_test_filtered = y_test[test_mask]\n",
    "\n",
    "                # Predict and calculate AUC\n",
    "                y_prob_filtered = model.predict(X_test_filtered).ravel()\n",
    "                if len(np.unique(y_test_filtered)) > 1:\n",
    "                    auc_category = roc_auc_score(y_test_filtered, y_prob_filtered)\n",
    "                    auc_by_category[category].append(auc_category)\n",
    "\n",
    "\n",
    "    if calculate_shap:\n",
    "        # Compute SHAP values for the test set of this fold\n",
    "        explainer = shap.DeepExplainer(model, np.array(X_train))\n",
    "        base_value = explainer.expected_value[0]\n",
    "        shap_value = explainer.shap_values(np.array(X_test))[0]\n",
    "\n",
    "    # Calculate metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    metrics_result, y_pred = calculate_metrics(y_test, y_prob, ci=ci)\n",
    "    metrics_result['Model'] = 'MLP'\n",
    "\n",
    "    if plot_roc:\n",
    "        plt.plot(fpr, tpr, label=f\"MLP AUC: {metrics_result['AUC']:.4f}\")\n",
    "\n",
    "    returned = []\n",
    "    returned.append(metrics_result)\n",
    "\n",
    "    if calculate_shap:\n",
    "        # compare the sum of shap values for each test sample and compare it with the output of the model\n",
    "        returned.append(shap_value)\n",
    "        returned.append(base_value)\n",
    "        returned.append(y_pred)\n",
    "\n",
    "    if by_category:\n",
    "        returned.append(auc_by_category)\n",
    "\n",
    "    return returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4979b4",
   "metadata": {},
   "source": [
    "## Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af194386",
   "metadata": {},
   "outputs": [],
   "source": [
    "traditional_models = [\n",
    "    'LogisticRegression', \n",
    "    'LightGBM', \n",
    "    'XGBoost',\n",
    "    'RandomForest',\n",
    "    'SVM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a89ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_traditional(model_name, X_train, X_test, y_train, y_test, selected_features, ci = False, plot_roc = False, calculate_shap = False, by_category = False):\n",
    "\n",
    "    '''\n",
    "    by default, the function will only return [0] metrics result\n",
    "    ci: whether to calculate the confidence interval\n",
    "    plot_roc: whether to plot the ROC curve\n",
    "    calculate_shap: whether to calculate the SHAP values, if true, will return:\n",
    "        [0] metrics result\n",
    "        [1] shap values\n",
    "        [2] base value\n",
    "        [3] y_pred\n",
    "    by_category: whether to calculate the AUC by each category, if true, will return:\n",
    "        [0] metrics result\n",
    "        [1] auc by each category\n",
    "    if calculate_shap and by_category, will return:\n",
    "        [0] metrics result\n",
    "        [1] shap values\n",
    "        [2] base value\n",
    "        [3] y_pred\n",
    "        [4] auc by each category\n",
    "    '''\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    auc_by_category = {0: [], 1: [], 2: []}\n",
    "\n",
    "    X_train, X_test, = X_train.loc[:, selected_features], X_test.loc[:, selected_features]\n",
    "\n",
    "    # sanitize column names to avoid error in LightGBM\n",
    "    X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "    X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n",
    "    if model_name == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=seed)\n",
    "    elif model_name == 'LightGBM':\n",
    "        model = lgb.LGBMClassifier(random_state=seed, verbose=-1)\n",
    "    elif model_name == 'XGBoost':\n",
    "        model = xgb.XGBClassifier(random_state=seed, verbosity=0)\n",
    "    elif model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier(random_state=seed)\n",
    "    elif model_name == 'SVM':\n",
    "        model = SVC(probability=True, random_state=seed)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model: {model_name}')\n",
    "        \n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    if by_category:\n",
    "        for category in auc_by_category:\n",
    "            test_mask = X_test['Surgery_type'] == category\n",
    "\n",
    "            if np.any(test_mask):\n",
    "                X_test_filtered = X_test[test_mask]\n",
    "                y_test_filtered = y_test[test_mask]\n",
    "\n",
    "                # Predict and calculate AUC\n",
    "                y_prob_filtered = model.predict_proba(X_test_filtered)[:, 1]\n",
    "\n",
    "                if len(np.unique(y_test_filtered)) > 1:\n",
    "                    auc_category = roc_auc_score(y_test_filtered, y_prob_filtered)\n",
    "                    auc_by_category[category].append(auc_category)\n",
    "\n",
    "    if calculate_shap:\n",
    "        if model_name == 'LogisticRegression':\n",
    "            explainer = shap.LinearExplainer(model, X_train)\n",
    "        elif model_name in ['LightGBM', 'XGBoost', 'RandomForest']:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "\n",
    "        base_value = explainer.expected_value[0] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap_value = shap_values[1] if len(shap_values) == 2 else shap_values\n",
    "\n",
    "\n",
    "\n",
    "    # use Youden's J Statistic to determine the optimal threshold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    metrics_result, y_pred = calculate_metrics(y_test, y_prob, ci=ci)\n",
    "    metrics_result['Model'] = model_name\n",
    "\n",
    "    if plot_roc:\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} AUC: {metrics_result['AUC']:.4f}\")\n",
    "\n",
    "    returned = []\n",
    "    returned.append(metrics_result)\n",
    "    \n",
    "    if calculate_shap:\n",
    "        returned.append(shap_value)\n",
    "        returned.append(base_value)\n",
    "        returned.append(y_pred)\n",
    "\n",
    "    if by_category:\n",
    "        returned.append(auc_by_category)\n",
    "\n",
    "\n",
    "    return returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e3fe1",
   "metadata": {},
   "source": [
    "# Model Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd153fe",
   "metadata": {},
   "source": [
    "## step 1: ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a60a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually select the fold index\n",
    "selected_index = 17966\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "train_index, test_index = train_test_split(df.index, test_size=0.2, random_state=selected_index, stratify=df['aki'])\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "X_train.loc[:, numeric_noTime] = scaler.fit_transform(X_train[numeric_noTime])\n",
    "X_test.loc[:, numeric_noTime] = scaler.transform(X_test[numeric_noTime])\n",
    "\n",
    "qualified, spreadsheet_df = test_splitting(original_df, train_index, test_index)\n",
    "\n",
    "metrics_result = assess_mlp(X_train, X_test, y_train, y_test, selected_features=X.columns, ci = True, plot_roc=True)[0]\n",
    "\n",
    "metrics_ci = pd.DataFrame([metrics_result])\n",
    "\n",
    "for model_name in traditional_models:\n",
    "    model_result = assess_traditional(model_name, X_train, X_test, y_train, y_test, selected_features=X.columns, ci = True, plot_roc=True)[0]\n",
    "    metrics_ci = pd.concat([metrics_ci, pd.DataFrame([model_result])], ignore_index=True)\n",
    "\n",
    "\n",
    "# reorder the columns and save only 3 decimal places\n",
    "metrics_ci = metrics_ci[['Model', 'AUC', 'AUC_CI', 'Recall', 'Recall_CI', 'Precision', 'Precision_CI', 'Accuracy', 'Accuracy_CI', 'Cutoff']]\n",
    "metrics_ci = metrics_ci.round(3)\n",
    "\n",
    "selected_model = 'LightGBM'\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Different Models')\n",
    "plt.grid(False)\n",
    "plt.legend()\n",
    "\n",
    "# save the plot in a folder named 'plots_split_{selected_index}'\n",
    "output_dir = f'plots_split_{selected_index}_{selected_model}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# clear all content in the folder\n",
    "for item in os.listdir(output_dir):\n",
    "    item_path = os.path.join(output_dir, item)\n",
    "    if os.path.isfile(item_path):\n",
    "        os.unlink(item_path)\n",
    "    elif os.path.isdir(item_path):\n",
    "        shutil.rmtree(item_path)\n",
    "    \n",
    "plt.savefig(f'plots_split_{selected_index}_{selected_model}/roc_curve.png', bbox_inches='tight')\n",
    "metrics_ci.to_excel(f'plots_split_{selected_index}_{selected_model}/metrics_ci_{selected_index}.xlsx', index=False)\n",
    "spreadsheet_df.to_excel(f'plots_split_{selected_index}_{selected_model}/train_test_split_{selected_index}.xlsx', index=False)\n",
    "\n",
    "\n",
    "metrics_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39799374",
   "metadata": {},
   "source": [
    "## step 2: SHAP for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap dot plot\n",
    "\n",
    "result = assess_traditional(selected_model, X_train, X_test, y_train, y_test, selected_features=X.columns, calculate_shap=True)\n",
    "\n",
    "metrics_result, shap_value, base_value, y_pred = result\n",
    "\n",
    "# Generate SHAP summary plot\n",
    "shap.summary_plot(shap_value, X_test, max_display=len(X.columns), plot_type='dot', show=False)\n",
    "\n",
    "plt.savefig(f'plots_split_{selected_index}_{selected_model}/shap_summary_plot.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed98f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap waterfall plot\n",
    "\n",
    "# Create an Explanation object\n",
    "patientNo = original_df['Patient No.']\n",
    "\n",
    "shap_explanation = shap.Explanation(\n",
    "    values=shap_value, \n",
    "    base_values= np.array([base_value[0] for _ in range(shap_value.shape[0])]),\n",
    "    data= original_df[X.columns].iloc[test_index, :])\n",
    "\n",
    "# plot all the waterfall plots and save the plots into a local folder named 'waterfall_plots'\n",
    "output_dir = f'plots_split_{selected_index}_{selected_model}/waterfall_plots_all_features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# clear all content in the folder\n",
    "for file in os.listdir(output_dir):\n",
    "    os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "for i in range(len(shap_explanation)):\n",
    "    # Generate the waterfall plot (SHAP will manage its own figure)\n",
    "    shap.waterfall_plot(shap_explanation[i], max_display=12, show=False)\n",
    "\n",
    "    # Get the current figure (created by SHAP)\n",
    "    fig = plt.gcf()\n",
    "    # Set the size of the figure\n",
    "    fig.set_size_inches(12, 6)\n",
    "    \n",
    "    # Add a label to the plot\n",
    "\n",
    "    label = f'Waterfall Plot of Patient No. {patientNo.iloc[test_index[i]]}: aki {int(y.iloc[test_index[i]])} prediction {int(y_pred[i])}'\n",
    "    plt.text(0.5, 0.95, label, \n",
    "             ha='center', va='center', transform=fig.transFigure, fontsize=16, fontweight='bold')\n",
    "    # Save the current figure\n",
    "    img_name = f'{output_dir}/aki_{int(y.iloc[test_index[i]])}_prediction_{int(y_pred[i])}_patientNo_{patientNo.iloc[test_index[i]]}.png'\n",
    "    fig.savefig(img_name, bbox_inches='tight')\n",
    "    # Close the figure to free up memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print('SHAP waterfall plots for all features are saved in the folder: ', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f224b",
   "metadata": {},
   "source": [
    "## step 3: feature selection curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "mean_shap_values = np.abs(shap_value).mean(axis=0)\n",
    "sorted_indices = np.argsort(mean_shap_values)\n",
    "sorted_indices = np.flip(sorted_indices)\n",
    "sorted_features = X.columns[sorted_indices]\n",
    "\n",
    "feature_selection_auc = []\n",
    "\n",
    "for num_features in range(1, len(X.columns)+1):\n",
    "    metrics_result = assess_traditional(selected_model, X_train, X_test, y_train, y_test, selected_features=sorted_features[:num_features])\n",
    "\n",
    "    auc = metrics_result[0]['AUC']\n",
    "\n",
    "    feature_selection_auc.append(auc)  \n",
    "\n",
    "# find the highest AUC and its column name in each row\n",
    "highest_auc = np.max(feature_selection_auc)\n",
    "highest_num = feature_selection_auc.index(highest_auc) + 1\n",
    "\n",
    "# find the list of features for the highest AUC and save it in the column named 'Best_Remaining_Features'\n",
    "best_features = sorted_features[:highest_num].tolist()\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature selection curve\n",
    "\n",
    "# Set the style for the chart\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\", font_scale=1.2)\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting with straight lines and dots\n",
    "plt.plot(range(1, len(X.columns)+1), feature_selection_auc, marker='o', linestyle='-', color='b', markersize=6, linewidth=2)\n",
    "\n",
    "# Set axis labels and chart title\n",
    "plt.xlabel(\"Number of Features\", fontsize=14)\n",
    "plt.ylabel(\"AUC\", fontsize=14)\n",
    "plt.title(\"AUC vs Number of Features\", fontsize=16)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot in a folder named 'plots_split_{selected_index}'\n",
    "\n",
    "plt.savefig(f'plots_split_{selected_index}_{selected_model}/feature_selection_curve.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1c256",
   "metadata": {},
   "source": [
    "## step 4: SHAP for best remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best remaining features\n",
    "best_with_intraoperative = best_features + intraoperative_features\n",
    "\n",
    "final_numeric = [col for col in numeric_cols if col in best_with_intraoperative]\n",
    "\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(new_X.loc[:, best_with_intraoperative], y, test_size=0.2, random_state=selected_index, stratify=y)\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "X_train_final.loc[:, final_numeric] = scaler.fit_transform(X_train_final.loc[:, final_numeric])\n",
    "X_test_final.loc[:, final_numeric] = scaler.transform(X_test_final.loc[:, final_numeric])\n",
    "\n",
    "result = assess_traditional(selected_model, X_train_final, X_test_final, y_train_final, y_test_final, selected_features=best_with_intraoperative, calculate_shap=True, by_category=True)\n",
    "\n",
    "metrics_result_best, shap_value_best, base_value_best, y_pred_best, auc_by_category_best = result\n",
    "\n",
    "# Generate SHAP summary plot\n",
    "shap.summary_plot(shap_value_best, X_test_final, max_display=len(best_with_intraoperative), plot_type='dot')\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(f'plots_split_{selected_index}_{selected_model}/shap_summary_plot_best.png', bbox_inches='tight')\n",
    "\n",
    "print('AUC by category: ', auc_by_category_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73287a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap waterfall plot\n",
    "\n",
    "# Create an Explanation object\n",
    "patientNo = original_df['Patient No.']\n",
    "\n",
    "shap_explanation_best = shap.Explanation(\n",
    "    values=shap_value_best, \n",
    "    base_values= np.array([base_value_best[0] for _ in range(shap_value_best.shape[0])]), \n",
    "    data= original_df[best_with_intraoperative].iloc[test_index, :])\n",
    "\n",
    "\n",
    "# plot all the waterfall plots and save the plots into a local folder named 'waterfall_plots'\n",
    "output_dir = f'plots_split_{selected_index}_{selected_model}/waterfall_plots_best_features+intraoperative'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# clear all content in the folder\n",
    "for file in os.listdir(output_dir):\n",
    "    os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "\n",
    "for i in range(len(shap_explanation_best)):\n",
    "    # Generate the waterfall plot (SHAP will manage its own figure)\n",
    "    shap.waterfall_plot(shap_explanation_best[i], max_display=12, show=False)\n",
    "\n",
    "    # Get the current figure (created by SHAP)\n",
    "    fig = plt.gcf()\n",
    "    # Set the size of the figure\n",
    "    fig.set_size_inches(12, 6)\n",
    "    \n",
    "    # Add a label to the plot\n",
    "\n",
    "    label = f'Waterfall Plot of Patient No. {patientNo.iloc[test_index[i]]}: aki {int(y.iloc[test_index[i]])} prediction {int(y_pred_best[i])}'\n",
    "    plt.text(0.5, 0.95, label, \n",
    "             ha='center', va='center', transform=fig.transFigure, fontsize=16, fontweight='bold')\n",
    "    # Save the current figure\n",
    "    img_name = f'{output_dir}/aki_{int(y.iloc[test_index[i]])}_prediction_{int(y_pred_best[i])}_patientNo_{patientNo.iloc[test_index[i]]}.png'\n",
    "    fig.savefig(img_name, bbox_inches='tight')\n",
    "    # Close the figure to free up memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print('SHAP waterfall plots for final features are saved in the folder: ', output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f38ab358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP dependence plots for each numerical feature\n",
    "n_cols = 3\n",
    "n_rows = (len(final_numeric) + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define labels for each subplot\n",
    "labels = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "\n",
    "for i, feature in enumerate(final_numeric):\n",
    "    sns.scatterplot(x=original_df.iloc[test_index, :][feature], y=shap_value_best[:, best_with_intraoperative.index(feature)], ax=axes[i], hue=y_test_final)\n",
    "    axes[i].set_title(f'SHAP Dependence Plot for {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('SHAP Value')\n",
    "    axes[i].grid(False)\n",
    "    \n",
    "    # Add label to the subplot\n",
    "    axes[i].text(-0.05, 1.1, labels[i], transform=axes[i].transAxes, \n",
    "                 fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f'plots_split_{selected_index}_{selected_model}/shap_dependence_plot_best.png', bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
